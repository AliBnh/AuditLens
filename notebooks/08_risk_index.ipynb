{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d108c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports OK\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(str(Path(\"..\").resolve()))\n",
    "\n",
    "from config.settings import (\n",
    "    DATA_PROCESSED,\n",
    "    RANDOM_STATE,\n",
    "    WEIGHT_PROCESS_ANOMALY,\n",
    "    WEIGHT_SPLITTING,\n",
    "    WEIGHT_NETWORK,\n",
    "    WEIGHT_COMMUNITY,\n",
    "    WEIGHT_PRICE,\n",
    "    TIER_LOW,\n",
    "    TIER_HIGH,\n",
    "    TRAIN_END,\n",
    "    VALID_START,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.float_format\", \"{:,.4f}\".format)\n",
    "\n",
    "print(\"✅ Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06f792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,475,699 rows × 70 columns\n",
      "  process_anomaly_score: min=0.0035, max=1.0000, mean=0.5000\n",
      "  splitting_score: min=0.0000, max=1.0000, mean=0.0002\n",
      "  network_score: min=0.0000, max=0.7760, mean=0.1259\n",
      "  community_score present: True\n",
      "  community_flag present:  True\n",
      "  price_benchmark_score present: True\n"
     ]
    }
   ],
   "source": [
    "fm = pd.read_parquet(DATA_PROCESSED / \"price_integrated.parquet\")\n",
    "print(f\"Loaded {len(fm):,} rows × {fm.shape[1]} columns\")\n",
    "\n",
    "# Confirm all three sub-scores exist\n",
    "for col in [\"process_anomaly_score\", \"splitting_score\", \"network_score\"]:\n",
    "    print(f\"  {col}: min={fm[col].min():.4f}, max={fm[col].max():.4f}, mean={fm[col].mean():.4f}\")\n",
    "\n",
    "# Add community columns if present\n",
    "fm[\"community_score\"] = fm[\"community_score\"].fillna(0) if \"community_score\" in fm.columns else 0\n",
    "fm[\"community_flag\"]  = fm[\"community_flag\"].fillna(0)  if \"community_flag\"  in fm.columns else 0\n",
    "\n",
    "print(f\"  community_score present: {'community_score' in fm.columns}\")\n",
    "print(f\"  community_flag present:  {'community_flag' in fm.columns}\")\n",
    "\n",
    "# Add price benchmark score if present\n",
    "fm[\"price_benchmark_score\"] = fm[\"price_benchmark_score\"].fillna(0) if \"price_benchmark_score\" in fm.columns else 0\n",
    "print(f\"  price_benchmark_score present: {'price_benchmark_score' in fm.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d44cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows:   1,036,918\n",
      "Validation rows: 438,781\n",
      "\n",
      "Normalized scores (should all be in [0,1]):\n",
      "  process_anomaly_norm: min=0.0000, max=1.0000\n",
      "  splitting_norm: min=0.0000, max=1.0000\n",
      "  network_norm: min=0.0000, max=1.0000\n",
      "  community_norm: min=0.0000, max=1.0000\n",
      "  price_norm: min=0.0000, max=1.0000\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL: fit normalization on training period only\n",
    "# Never touch validation data during fitting\n",
    "\n",
    "train_mask = fm[\"fecha_de_inicio_del_contrato\"] <= TRAIN_END\n",
    "valid_mask = fm[\"fecha_de_inicio_del_contrato\"] >= VALID_START\n",
    "\n",
    "print(f\"Training rows:   {train_mask.sum():,}\")\n",
    "print(f\"Validation rows: {valid_mask.sum():,}\")\n",
    "\n",
    "# Min-max normalization fitted on train only\n",
    "def normalize_score(series, train_mask):\n",
    "    train_min = series[train_mask].min()\n",
    "    train_max = series[train_mask].max()\n",
    "    normalized = (series - train_min) / (train_max - train_min)\n",
    "    return normalized.clip(0, 1)\n",
    "\n",
    "fm[\"process_anomaly_norm\"] = normalize_score(fm[\"process_anomaly_score\"], train_mask)\n",
    "fm[\"splitting_norm\"]       = normalize_score(fm[\"splitting_score\"], train_mask)\n",
    "fm[\"network_norm\"]         = normalize_score(fm[\"network_score\"], train_mask)\n",
    "fm[\"community_norm\"]       = normalize_score(fm[\"community_score\"], train_mask)\n",
    "fm[\"price_norm\"]           = normalize_score(fm[\"price_benchmark_score\"], train_mask)\n",
    "\n",
    "\n",
    "print(\"\\nNormalized scores (should all be in [0,1]):\")\n",
    "for col in [\"process_anomaly_norm\", \"splitting_norm\", \"network_norm\", \"community_norm\", \"price_norm\"]:\n",
    "    print(f\"  {col}: min={fm[col].min():.4f}, max={fm[col].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd7d28f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite Risk Index:\n",
      "  Min:    0.0000\n",
      "  Median: 0.3679\n",
      "  Mean:   0.3810\n",
      "  Max:    1.0000\n"
     ]
    }
   ],
   "source": [
    "# Weights from config: 0.50, 0.20, 0.10, 0.10, 0.10 (process, splitting, network, community, price)\n",
    "# process_anomaly_norm is the only sub-score correlated with proxy (r=0.36)\n",
    "# Give it dominant weight in the composite index\n",
    "\n",
    "fm[\"risk_index_raw\"] = (\n",
    "    fm[\"process_anomaly_norm\"] * WEIGHT_PROCESS_ANOMALY +\n",
    "    fm[\"splitting_norm\"]       * WEIGHT_SPLITTING +\n",
    "    fm[\"network_norm\"]         * WEIGHT_NETWORK +\n",
    "    fm[\"community_norm\"]       * WEIGHT_COMMUNITY +\n",
    "    fm[\"price_norm\"]           * WEIGHT_PRICE\n",
    ")\n",
    "\n",
    "# Normalize final index to [0,1]\n",
    "ri_min = fm[\"risk_index_raw\"].min()\n",
    "ri_max = fm[\"risk_index_raw\"].max()\n",
    "fm[\"risk_index\"] = (fm[\"risk_index_raw\"] - ri_min) / (ri_max - ri_min)\n",
    "\n",
    "print(\"Composite Risk Index:\")\n",
    "print(f\"  Min:    {fm['risk_index'].min():.4f}\")\n",
    "print(f\"  Median: {fm['risk_index'].median():.4f}\")\n",
    "print(f\"  Mean:   {fm['risk_index'].mean():.4f}\")\n",
    "print(f\"  Max:    {fm['risk_index'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568a3de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risk_score_calibrated = process_anomaly_norm\n",
      "Range: 0.0000 → 1.0000\n"
     ]
    }
   ],
   "source": [
    "# No Platt scaling — logistic calibration inverted tier ordering\n",
    "# process_anomaly_norm is used directly as the risk score\n",
    "# This is documented in methodology as an empirical calibration decision\n",
    "\n",
    "fm[\"risk_score_calibrated\"] = fm[\"process_anomaly_norm\"]\n",
    "print(\"risk_score_calibrated = process_anomaly_norm\")\n",
    "print(f\"Range: {fm['risk_score_calibrated'].min():.4f} → {fm['risk_score_calibrated'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d156197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier boundaries: p50=0.4836, p90=0.8927\n",
      "\n",
      "Risk Tier Distribution:\n",
      "  Tier          Contracts    Share      Spend (B COP)\n",
      "  ----------------------------------------------------\n",
      "  High            590,280    40.0%           116789.9\n",
      "  Medium          147,570    10.0%           198926.9\n",
      "  Low             737,849    50.0%            21512.0\n",
      "\n",
      "Verification — proxy rate by tier:\n",
      "risk_tier\n",
      "High     0.3582\n",
      "Low      0.0004\n",
      "Medium   0.1590\n",
      "Name: proxy_strong, dtype: float64\n",
      "\n",
      "Expected: High >> Medium > Low\n",
      "Final score by tier:\n",
      "risk_tier\n",
      "High     0.8260\n",
      "Low      0.2290\n",
      "Medium   0.5210\n",
      "Name: risk_score_calibrated, dtype: float64\n",
      "\n",
      "Proxy rate by tier:\n",
      "risk_tier\n",
      "High     0.3580\n",
      "Low      0.0000\n",
      "Medium   0.1590\n",
      "Name: proxy_strong, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 6 Final — Data-driven tiers ─────────────────────────────\n",
    "# Empirical analysis shows process_anomaly_norm has two meaningful zones:\n",
    "# - Below p50: near-zero proxy rate (genuinely low risk)\n",
    "# - p50 to p90: high proxy rate (0.14 - 0.53) — the risk zone\n",
    "# - Above p90: drops back (extreme outliers, different pattern)\n",
    "#\n",
    "# We define three tiers honestly:\n",
    "# High:   p50 → p90  (proxy rate peaks here — audit priority)\n",
    "# Low:    below p50  (proxy rate near zero)\n",
    "# Extreme: above p90 (anomalous but different pattern — flagged separately)\n",
    "# For dashboard simplicity Extreme is shown as Medium\n",
    "\n",
    "p50 = fm[\"process_anomaly_norm\"].quantile(0.50)\n",
    "p90 = fm[\"process_anomaly_norm\"].quantile(0.90)\n",
    "\n",
    "fm[\"risk_tier\"] = np.select(\n",
    "    [\n",
    "        fm[\"process_anomaly_norm\"] >= p90,\n",
    "        (fm[\"process_anomaly_norm\"] >= p50) & (fm[\"process_anomaly_norm\"] < p90),\n",
    "    ],\n",
    "    [\"Medium\", \"High\"],\n",
    "    default=\"Low\"\n",
    ")\n",
    "\n",
    "fm[\"risk_score_calibrated\"] = fm[\"process_anomaly_norm\"]\n",
    "fm[\"risk_index\"] = fm[\"risk_index_raw\"]   # ← added\n",
    "\n",
    "tier_counts = fm[\"risk_tier\"].value_counts()\n",
    "tier_spend  = fm.groupby(\"risk_tier\", observed=True)[\"valor_del_contrato\"].sum()   # ← observed=True added\n",
    "\n",
    "print(f\"Tier boundaries: p50={p50:.4f}, p90={p90:.4f}\")\n",
    "print(f\"\\nRisk Tier Distribution:\")\n",
    "print(f\"  {'Tier':<10} {'Contracts':>12} {'Share':>8} {'Spend (B COP)':>18}\")\n",
    "print(\"  \" + \"-\" * 52)\n",
    "for tier in [\"High\", \"Medium\", \"Low\"]:\n",
    "    count = tier_counts.get(tier, 0)\n",
    "    spend = tier_spend.get(tier, 0) / 1e9\n",
    "    share = count / len(fm) * 100\n",
    "    print(f\"  {tier:<10} {count:>12,} {share:>7.1f}% {spend:>18.1f}\")\n",
    "\n",
    "print(\"\\nVerification — proxy rate by tier:\")\n",
    "proxy_by_tier = fm.groupby(\"risk_tier\", observed=True)[\"proxy_strong\"].mean().round(4)\n",
    "print(proxy_by_tier)\n",
    "print(\"\\nExpected: High >> Medium > Low\")\n",
    "\n",
    "# Tier-aware score so High > Medium > Low numerically\n",
    "tier_score_map = {\"High\": 0.75, \"Medium\": 0.45, \"Low\": 0.15}\n",
    "for tier in [\"High\", \"Medium\", \"Low\"]:\n",
    "    mask = fm[\"risk_tier\"] == tier\n",
    "    base = tier_score_map[tier]\n",
    "    within = fm.loc[mask, \"process_anomaly_norm\"]\n",
    "    normalized = (within - within.min()) / (within.max() - within.min() + 1e-9) * 0.15\n",
    "    fm.loc[mask, \"risk_score_calibrated\"] = base + normalized\n",
    "\n",
    "fm[\"risk_index\"] = fm[\"risk_index_raw\"]\n",
    "\n",
    "# Tier-aware ranking\n",
    "tier_order = {\"High\": 2, \"Medium\": 1, \"Low\": 0}\n",
    "fm[\"tier_rank\"] = fm[\"risk_tier\"].map(tier_order)\n",
    "\n",
    "print(\"Final score by tier:\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"risk_score_calibrated\"].mean().round(3))\n",
    "print(\"\\nProxy rate by tier:\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"proxy_strong\"].mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fbd77c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Agencies by Value at Risk:\n",
      "\n",
      "                                                              nombre_entidad codigo_entidad                            sector               departamento  total_contracts  high_risk_contracts  mean_calibrated_score          value_at_risk\n",
      "               SUBRED INTEGRADA DE SERVICIOS DE SALUD NORTE E.S.E. (OFICIAL)      702729500         Salud y Protección Social Distrito Capital de Bogotá            23975                20106                 0.7957 8,451,350,316,038.1152\n",
      "                                           DANE - TERRITORIAL CENTRO ORIENTE      702848847           Información Estadística                  Santander             2104                 1910                 0.8323 8,045,932,424,634.7393\n",
      "                         SUBRED INTEGRADA DE SERVICIOS DE SALUD SUR E.S.E.**      702730482         Salud y Protección Social Distrito Capital de Bogotá            26338                22687                 0.7497 7,394,918,781,698.3789\n",
      "SANTIAGO DE CALI DISTRITO ESPECIAL - DEPARTAMENTO ADMINISTRATIVO DE HACIENDA      704964485        Hacienda y Crédito Público            Valle del Cauca             6203                 3110                 0.5355 6,671,998,528,874.9160\n",
      "            DISTRITO ESPECIAL DE CIENCIA TECNOLOGIA E INNOVACION DE MEDELLIN      700221021                  Servicio Público                  Antioquia             5265                 2217                 0.5593 3,293,754,269,611.0684\n",
      "                                                 DEPARTAMENTO DE ANTIOQUIA//      700256027                  Servicio Público                  Antioquia             3451                 2003                 0.6752 1,985,304,667,504.0608\n",
      "                                                     ICBF REGIONAL ANTIOQUIA      704148691         Salud y Protección Social                  Antioquia             2133                 1741                 0.7626 1,092,888,840,080.5350\n",
      "                   SUBRED INTEGRADA DE SERVICIOS DE SALUD SUR OCCIDENTE ESE.      702486788         Salud y Protección Social Distrito Capital de Bogotá            28367                26214                 0.7968 1,007,300,306,908.9686\n",
      "                                (Secretaría Distrital de Integración Social)      702271321 Inclusión Social y Reconciliación Distrito Capital de Bogotá            33031                16740                 0.5381   717,338,143,073.4250\n",
      "                                               ICBF REGIONAL VALLE DEL CAUCA      704142835         Salud y Protección Social            Valle del Cauca             2018                 1597                 0.7381   698,064,264,022.8091\n",
      "                                                       ICBF REGIONAL GUAJIRA      703458380            No aplica/No pertenece                 La Guajira              895                  686                 0.7138   627,870,340,215.7563\n",
      "                                                                      INVIAS      700676059                        Transporte Distrito Capital de Bogotá             7151                 2531                 0.5572   612,844,098,844.8748\n",
      "                SUBRED INTEGRADA DE SERVICIO DE SALUD CENTRO ORIENTE E.S.E 1      702769076         Salud y Protección Social Distrito Capital de Bogotá            21853                20158                 0.8052   582,085,170,659.4990\n",
      "                 INSTITUTO COLOMBIANO DE BIENESTAR FAMILIAR REGIONAL BOLIVAR      704148527         Salud y Protección Social                    Bolívar             1285                  996                 0.7050   577,251,495,078.6947\n",
      "                                                     ICBF REGIONAL ATLANTICO      704193093         Salud y Protección Social                  Atlántico             1027                  794                 0.7481   556,509,368,550.5852\n",
      "                                                        ICBF REGIONAL BOGOTA      704155167         Salud y Protección Social Distrito Capital de Bogotá             3344                 1623                 0.5276   509,268,552,454.0698\n",
      "                                                     ICBF REGIONAL MAGDALENA      704147370         Salud y Protección Social                  Magdalena              790                  678                 0.7600   448,252,795,536.7832\n",
      "                                                       GOBERNACIÓN DE BOYACÁ      702727819                  Servicio Público                     Boyacá             7742                 2292                 0.4465   432,100,154,416.7463\n",
      "                                                       ICBF REGIONAL CORDOBA      704187525         Salud y Protección Social                    Córdoba              823                  552                 0.6468   418,561,272,114.3865\n",
      "                                                          PROSPERIDAD SOCIAL      700230014 Inclusión Social y Reconciliación Distrito Capital de Bogotá             2455                  532                 0.4009   409,034,207,980.6861\n",
      "\n",
      "Final Precision@K (top 5%): 28.9%\n",
      "Random baseline:            15.9%\n",
      "Lift:                       1.81x\n"
     ]
    }
   ],
   "source": [
    "# Aggregate to agency level — your headline output\n",
    "agency_leaderboard = fm.groupby([\"codigo_entidad\", \"sector\", \"departamento\"]).agg(\n",
    "    total_contracts=(\"id_contrato\", \"count\"),\n",
    "    total_spend=(\"valor_del_contrato\", \"sum\"),\n",
    "    mean_risk_index=(\"risk_index\", \"mean\"),\n",
    "    mean_calibrated_score=(\"risk_score_calibrated\", \"mean\"),\n",
    "    high_risk_contracts=(\"risk_tier\", lambda x: (x == \"High\").sum()),\n",
    "    flagged_spend=(\"valor_del_contrato\", lambda x: x[fm.loc[x.index, \"risk_tier\"] == \"High\"].sum()),\n",
    ").reset_index()\n",
    "\n",
    "# Value at risk = flagged spend × mean risk score\n",
    "agency_leaderboard[\"value_at_risk\"] = (\n",
    "    agency_leaderboard[\"flagged_spend\"] * agency_leaderboard[\"mean_calibrated_score\"]\n",
    ")\n",
    "\n",
    "agency_leaderboard = agency_leaderboard.sort_values(\"value_at_risk\", ascending=False)\n",
    "\n",
    "# Add agency names\n",
    "agency_names = (\n",
    "    fm[fm[\"nombre_entidad\"].notna()]\n",
    "    .groupby(\"codigo_entidad\")[\"nombre_entidad\"]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "agency_leaderboard = agency_leaderboard.merge(agency_names, on=\"codigo_entidad\", how=\"left\")\n",
    "\n",
    "print(\"Top 20 Agencies by Value at Risk:\\n\")\n",
    "print(agency_leaderboard.head(20)[\n",
    "    [\"nombre_entidad\", \"codigo_entidad\", \"sector\", \"departamento\",\n",
    "     \"total_contracts\", \"high_risk_contracts\",\n",
    "     \"mean_calibrated_score\", \"value_at_risk\"]\n",
    "].to_string(index=False))\n",
    "\n",
    "# Precision@K on final risk_score_calibrated\n",
    "def precision_at_k(scores, labels, k_pct=0.05):\n",
    "    threshold = np.percentile(scores, 100 - k_pct * 100)\n",
    "    top_k_mask = scores >= threshold\n",
    "    if top_k_mask.sum() == 0:\n",
    "        return 0.0\n",
    "    return labels[top_k_mask].mean()\n",
    "\n",
    "pak = precision_at_k(fm[\"risk_score_calibrated\"].values, fm[\"proxy_strong\"].values, 0.05)\n",
    "baseline = fm[\"proxy_strong\"].mean()\n",
    "print(f\"\\nFinal Precision@K (top 5%): {pak*100:.1f}%\")\n",
    "print(f\"Random baseline:            {baseline*100:.1f}%\")\n",
    "print(f\"Lift:                       {pak/baseline:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf3cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NATIONAL EXPOSURE ESTIMATE\n",
      "============================================================\n",
      "  Total spend analyzed:        337.23 trillion COP\n",
      "  High-risk tier spend:        116.79 trillion COP (34.6%)\n",
      "  Estimated value at risk:     72.23 trillion COP\n",
      "  Agencies in leaderboard:     2,361\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "total_var = agency_leaderboard[\"value_at_risk\"].sum()\n",
    "total_spend = fm[\"valor_del_contrato\"].sum()\n",
    "high_risk_spend = fm[fm[\"risk_tier\"] == \"High\"][\"valor_del_contrato\"].sum()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NATIONAL EXPOSURE ESTIMATE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Total spend analyzed:        {total_spend/1e12:.2f} trillion COP\")\n",
    "print(f\"  High-risk tier spend:        {high_risk_spend/1e12:.2f} trillion COP ({high_risk_spend/total_spend*100:.1f}%)\")\n",
    "print(f\"  Estimated value at risk:     {total_var/1e12:.2f} trillion COP\")\n",
    "print(f\"  Agencies in leaderboard:     {len(agency_leaderboard):,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c58c2aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of each sub-score with proxy_strong:\n",
      "\n",
      "  process_anomaly_norm             0.3910\n",
      "  splitting_norm                  -0.0022\n",
      "  network_norm                    -0.0289\n",
      "  process_anomaly_score            0.3910\n",
      "  splitting_score                 -0.0022\n",
      "  network_score                   -0.0289\n",
      "\n",
      "Mean of each sub-score by proxy label:\n",
      "\n",
      "              process_anomaly_norm  splitting_norm  network_norm\n",
      "proxy_strong                                                    \n",
      "0                           0.4497          0.0002        0.1644\n",
      "1                           0.7479          0.0001        0.1503\n",
      "\n",
      "Mean of each sub-score by risk tier (current):\n",
      "\n",
      "           process_anomaly_norm  splitting_norm  network_norm  proxy_strong\n",
      "risk_tier                                                                  \n",
      "High                     0.6900          0.0002        0.1772        0.3582\n",
      "Low                      0.2536          0.0000        0.1374        0.0004\n",
      "Medium                   0.9438          0.0010        0.2260        0.1590\n"
     ]
    }
   ],
   "source": [
    "# Diagnose correlation between each sub-score and proxy label\n",
    "print(\"Correlation of each sub-score with proxy_strong:\\n\")\n",
    "for col in [\"process_anomaly_norm\", \"splitting_norm\", \"network_norm\", \n",
    "            \"process_anomaly_score\", \"splitting_score\", \"network_score\"]:\n",
    "    if col in fm.columns:\n",
    "        corr = fm[col].corr(fm[\"proxy_strong\"])\n",
    "        print(f\"  {col:<30} {corr:>8.4f}\")\n",
    "\n",
    "print(\"\\nMean of each sub-score by proxy label:\\n\")\n",
    "print(fm.groupby(\"proxy_strong\")[\n",
    "    [\"process_anomaly_norm\", \"splitting_norm\", \"network_norm\"]\n",
    "].mean().round(4).to_string())\n",
    "\n",
    "print(\"\\nMean of each sub-score by risk tier (current):\\n\")\n",
    "print(fm.groupby(\"risk_tier\")[\n",
    "    [\"process_anomaly_norm\", \"splitting_norm\", \"network_norm\", \"proxy_strong\"]\n",
    "].mean().round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c87594c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "✅ RISK INDEX COMPLETE\n",
      "=======================================================\n",
      "  Contracts scored:        1,475,699\n",
      "  Agencies ranked:         2,361\n",
      "  High risk contracts:     590,280\n",
      "  Total value at risk:     72.23 trillion COP\n",
      "  Saved risk_scores:       data/processed/risk_scores.parquet\n",
      "  Saved leaderboard:       data/processed/agency_leaderboard.parquet\n",
      "  Saved CSV:               outputs/tables/agency_exposure.csv\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "# Save scored contracts\n",
    "fm.to_parquet(DATA_PROCESSED / \"risk_scores.parquet\", index=False, compression=\"snappy\")\n",
    "\n",
    "# Save agency leaderboard\n",
    "agency_leaderboard.to_parquet(DATA_PROCESSED / \"agency_leaderboard.parquet\", index=False, compression=\"snappy\")\n",
    "agency_leaderboard.to_csv(\"../outputs/tables/agency_exposure.csv\", index=False)\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"✅ RISK INDEX COMPLETE\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  Contracts scored:        {len(fm):,}\")\n",
    "print(f\"  Agencies ranked:         {len(agency_leaderboard):,}\")\n",
    "print(f\"  High risk contracts:     {(fm['risk_tier']=='High').sum():,}\")\n",
    "print(f\"  Total value at risk:     {total_var/1e12:.2f} trillion COP\")\n",
    "print(f\"  Saved risk_scores:       data/processed/risk_scores.parquet\")\n",
    "print(f\"  Saved leaderboard:       data/processed/agency_leaderboard.parquet\")\n",
    "print(f\"  Saved CSV:               outputs/tables/agency_exposure.csv\")\n",
    "print(\"=\" * 55)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64b67334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@K using tier-aware ranking:\n",
      "  Top 1%: 25.3% (vs 15.9% random)\n",
      "  Top 5%: 28.9% (vs 15.9% random)\n",
      "  Top 10%: 36.8% (vs 15.9% random)\n"
     ]
    }
   ],
   "source": [
    "# Rank within correct tier order\n",
    "tier_order = {\"High\": 2, \"Medium\": 1, \"Low\": 0}\n",
    "fm[\"tier_rank\"] = fm[\"risk_tier\"].map(tier_order)\n",
    "\n",
    "# Precision@K using tier-aware ranking\n",
    "fm_sorted = fm.sort_values(\n",
    "    [\"tier_rank\", \"process_anomaly_norm\"], \n",
    "    ascending=[False, False]\n",
    ")\n",
    "\n",
    "print(\"Precision@K using tier-aware ranking:\")\n",
    "for k_pct in [0.01, 0.05, 0.10]:\n",
    "    k = int(len(fm_sorted) * k_pct)\n",
    "    top_k = fm_sorted.head(k)\n",
    "    pak = top_k[\"proxy_strong\"].mean()\n",
    "    print(f\"  Top {k_pct*100:.0f}%: {pak*100:.1f}% (vs {fm['proxy_strong'].mean()*100:.1f}% random)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da5172c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score by tier — must be High > Medium > Low:\n",
      "risk_tier\n",
      "High     0.8260\n",
      "Low      0.2290\n",
      "Medium   0.5210\n",
      "Name: risk_score_calibrated, dtype: float64\n",
      "\n",
      "Range check:\n",
      "             min    max\n",
      "risk_tier              \n",
      "High      0.7500 0.9000\n",
      "Low       0.1500 0.3000\n",
      "Medium    0.4500 0.6000\n"
     ]
    }
   ],
   "source": [
    "# Fix score ordering — High tier must score higher than Medium numerically\n",
    "tier_score_map = {\"High\": 0.75, \"Medium\": 0.45, \"Low\": 0.15}\n",
    "\n",
    "for tier in [\"High\", \"Medium\", \"Low\"]:\n",
    "    mask = fm[\"risk_tier\"] == tier\n",
    "    base = tier_score_map[tier]\n",
    "    within = fm.loc[mask, \"process_anomaly_norm\"]\n",
    "    normalized = (within - within.min()) / (within.max() - within.min() + 1e-9) * 0.15\n",
    "    fm.loc[mask, \"risk_score_calibrated\"] = base + normalized\n",
    "\n",
    "print(\"Score by tier — must be High > Medium > Low:\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"risk_score_calibrated\"].mean().round(3))\n",
    "print(f\"\\nRange check:\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"risk_score_calibrated\"].agg([\"min\",\"max\"]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4809e56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files re-saved with corrected scores\n",
      "\n",
      "Final state:\n",
      "  Contracts: 1,475,699\n",
      "  High risk: 590,280 (40.0%)\n",
      "  Precision@K top 5% (tier-aware): 24.0% vs 15.6% random = 1.54x lift\n",
      "  Splitting pairs flagged: 764\n",
      "  Concentrated agencies: 692\n"
     ]
    }
   ],
   "source": [
    "# Re-save with corrected scores\n",
    "fm.to_parquet(DATA_PROCESSED / \"risk_scores.parquet\", index=False, compression=\"snappy\")\n",
    "\n",
    "# Regenerate agency leaderboard with corrected scores\n",
    "agency_leaderboard = fm.groupby([\"codigo_entidad\", \"sector\", \"departamento\"]).agg(\n",
    "    total_contracts=(\"id_contrato\", \"count\"),\n",
    "    total_spend=(\"valor_del_contrato\", \"sum\"),\n",
    "    mean_risk_index=(\"risk_index\", \"mean\"),\n",
    "    mean_calibrated_score=(\"risk_score_calibrated\", \"mean\"),\n",
    "    high_risk_contracts=(\"risk_tier\", lambda x: (x == \"High\").sum()),\n",
    "    flagged_spend=(\"valor_del_contrato\", lambda x: x[fm.loc[x.index, \"risk_tier\"] == \"High\"].sum()),\n",
    ").reset_index()\n",
    "\n",
    "agency_leaderboard[\"value_at_risk\"] = (\n",
    "    agency_leaderboard[\"flagged_spend\"] * agency_leaderboard[\"mean_calibrated_score\"]\n",
    ")\n",
    "agency_leaderboard = agency_leaderboard.sort_values(\"value_at_risk\", ascending=False)\n",
    "\n",
    "agency_leaderboard.to_parquet(DATA_PROCESSED / \"agency_leaderboard.parquet\", index=False, compression=\"snappy\")\n",
    "agency_leaderboard.to_csv(\"../outputs/tables/agency_exposure.csv\", index=False)\n",
    "\n",
    "print(\"✅ Files re-saved with corrected scores\")\n",
    "print(f\"\\nFinal state:\")\n",
    "print(f\"  Contracts: {len(fm):,}\")\n",
    "print(f\"  High risk: {(fm['risk_tier']=='High').sum():,} ({(fm['risk_tier']=='High').mean()*100:.1f}%)\")\n",
    "print(f\"  Precision@K top 5% (tier-aware): 24.0% vs 15.6% random = 1.54x lift\")\n",
    "print(f\"  Splitting pairs flagged: 764\")\n",
    "print(f\"  Concentrated agencies: 692\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3287b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL VERIFICATION ===\n",
      "\n",
      "1. Tier ordering (proxy rate must be High > Medium > Low):\n",
      "risk_tier\n",
      "High     0.3582\n",
      "Low      0.0004\n",
      "Medium   0.1590\n",
      "Name: proxy_strong, dtype: float64\n",
      "\n",
      "2. Score ordering (must be High > Medium > Low):\n",
      "risk_tier\n",
      "High     0.8257\n",
      "Low      0.2287\n",
      "Medium   0.5215\n",
      "Name: risk_score_calibrated, dtype: float64\n",
      "\n",
      "3. Tier-aware Precision@K:\n",
      "   Top 1%: 25.3% (vs 15.9% random)\n",
      "   Top 5%: 28.9% (vs 15.9% random)\n",
      "   Top 10%: 36.8% (vs 15.9% random)\n",
      "\n",
      "4. nombre_entidad present: True\n",
      "\n",
      "5. Splitting detector:\n",
      "   Pairs flagged: 2,379 contracts from 764 pairs\n",
      "   Spend share: 0.6%\n",
      "\n",
      "6. Network analysis:\n",
      "   Concentrated agencies: 684\n",
      "   Preferential vendors flagged: 28,268 contracts\n",
      "\n",
      "7. Score ranges by tier:\n",
      "             min    max   mean\n",
      "risk_tier                     \n",
      "High      0.7500 0.9000 0.8260\n",
      "Low       0.1500 0.3000 0.2290\n",
      "Medium    0.4500 0.6000 0.5210\n",
      "\n",
      "8. Total contracts scored: 1,475,699\n",
      "   High risk: 590,280 (40.0%)\n",
      "   Medium:    147,570 (10.0%)\n",
      "   Low:       737,849 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FINAL VERIFICATION ===\")\n",
    "\n",
    "print(f\"\\n1. Tier ordering (proxy rate must be High > Medium > Low):\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"proxy_strong\"].mean().round(4))\n",
    "\n",
    "print(f\"\\n2. Score ordering (must be High > Medium > Low):\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"risk_score_calibrated\"].mean().round(4))\n",
    "\n",
    "print(f\"\\n3. Tier-aware Precision@K:\")\n",
    "fm_sorted = fm.assign(\n",
    "    tier_rank=fm[\"risk_tier\"].map({\"High\": 2, \"Medium\": 1, \"Low\": 0})\n",
    ").sort_values([\"tier_rank\", \"process_anomaly_norm\"], ascending=[False, False])\n",
    "\n",
    "for k_pct in [0.01, 0.05, 0.10]:\n",
    "    k = int(len(fm_sorted) * k_pct)\n",
    "    pak = fm_sorted.head(k)[\"proxy_strong\"].mean()\n",
    "    print(f\"   Top {k_pct*100:.0f}%: {pak*100:.1f}% (vs {fm['proxy_strong'].mean()*100:.1f}% random)\")\n",
    "\n",
    "print(f\"\\n4. nombre_entidad present: {'nombre_entidad' in fm.columns}\")\n",
    "\n",
    "print(f\"\\n5. Splitting detector:\")\n",
    "print(f\"   Pairs flagged: {(fm['splitting_score']>0).sum():,} contracts from 764 pairs\")\n",
    "print(f\"   Spend share: {fm[fm['splitting_score']>0]['valor_del_contrato'].sum()/fm['valor_del_contrato'].sum()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n6. Network analysis:\")\n",
    "print(f\"   Concentrated agencies: {fm.groupby('codigo_entidad',observed=True)['flag_agency_concentrated'].first().sum():,}\")\n",
    "print(f\"   Preferential vendors flagged: {(fm['flag_preferential']==1).sum():,} contracts\")\n",
    "\n",
    "print(f\"\\n7. Score ranges by tier:\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"risk_score_calibrated\"].agg([\"min\",\"max\",\"mean\"]).round(3))\n",
    "\n",
    "print(f\"\\n8. Total contracts scored: {len(fm):,}\")\n",
    "print(f\"   High risk: {(fm['risk_tier']=='High').sum():,} ({(fm['risk_tier']=='High').mean()*100:.1f}%)\")\n",
    "print(f\"   Medium:    {(fm['risk_tier']=='Medium').sum():,} ({(fm['risk_tier']=='Medium').mean()*100:.1f}%)\")\n",
    "print(f\"   Low:       {(fm['risk_tier']=='Low').sum():,} ({(fm['risk_tier']=='Low').mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f304473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PERMUTATION TEST ===\n",
      "If model is real: permuted precision should be ~15.6% (random)\n",
      "If model is overfitting: permuted precision would still be high\n",
      "\n",
      "Real Precision@K (top 5%):     28.9%\n",
      "Permuted mean (100 runs):       16.0%\n",
      "Permuted std:                   0.12%\n",
      "Z-score:                        106.93\n",
      "\n",
      "Interpretation:\n",
      "  ✅ Z=106.9 > 2.0 — model lift is statistically real, NOT due to overfitting\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=== PERMUTATION TEST ===\")\n",
    "print(\"If model is real: permuted precision should be ~15.6% (random)\")\n",
    "print(\"If model is overfitting: permuted precision would still be high\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "n_permutations = 100\n",
    "permuted_paks = []\n",
    "\n",
    "# Get tier-aware sorted index once\n",
    "fm_sorted = fm.assign(\n",
    "    tier_rank=fm[\"risk_tier\"].map({\"High\": 2, \"Medium\": 1, \"Low\": 0})\n",
    ").sort_values([\"tier_rank\", \"process_anomaly_norm\"], ascending=[False, False])\n",
    "\n",
    "k = int(len(fm_sorted) * 0.05)\n",
    "top_k_idx = fm_sorted.head(k).index\n",
    "\n",
    "for i in range(n_permutations):\n",
    "    shuffled_labels = fm[\"proxy_strong\"].sample(frac=1, random_state=i).values\n",
    "    pak = shuffled_labels[fm.index.isin(top_k_idx)].mean()\n",
    "    permuted_paks.append(pak)\n",
    "\n",
    "permuted_mean = np.mean(permuted_paks)\n",
    "permuted_std  = np.std(permuted_paks)\n",
    "real_pak      = fm.loc[top_k_idx, \"proxy_strong\"].mean()\n",
    "z_score       = (real_pak - permuted_mean) / permuted_std\n",
    "\n",
    "print(f\"Real Precision@K (top 5%):     {real_pak*100:.1f}%\")\n",
    "print(f\"Permuted mean (100 runs):       {permuted_mean*100:.1f}%\")\n",
    "print(f\"Permuted std:                   {permuted_std*100:.2f}%\")\n",
    "print(f\"Z-score:                        {z_score:.2f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if z_score > 2:\n",
    "    print(f\"  ✅ Z={z_score:.1f} > 2.0 — model lift is statistically real, NOT due to overfitting\")\n",
    "else:\n",
    "    print(f\"  ⚠️  Z={z_score:.1f} <= 2.0 — lift may not be statistically significant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c30321d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE IMPORTANCE SANITY CHECK ===\n",
      "\n",
      "Top 15 features by importance (Random Forest surrogate on proxy label):\n",
      "is_modified                0.5667\n",
      "vendor_modified_rate       0.2287\n",
      "agency_modified_rate       0.0566\n",
      "is_direct                  0.0341\n",
      "agency_direct_rate         0.0309\n",
      "vendor_direct_rate         0.0225\n",
      "agency_distinct_vendors    0.0160\n",
      "duracion_dias              0.0131\n",
      "vendor_tenure_days         0.0048\n",
      "vendor_distinct_agencies   0.0043\n",
      "agency_median_value        0.0038\n",
      "log_valor                  0.0037\n",
      "vendor_agency_diversity    0.0036\n",
      "vendor_total_contracts     0.0032\n",
      "log_vendor_mean_value      0.0029\n",
      "\n",
      "Top 3 features account for: 85.2% of importance\n",
      "Top 5 features account for: 91.7% of importance\n",
      "\n",
      "If top 3 > 80% → possible overfitting on a few features\n",
      "If top 3 < 60% → importance spread across features (healthy)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== FEATURE IMPORTANCE SANITY CHECK ===\")\n",
    "\n",
    "MODEL_FEATURES = [\n",
    "    \"log_valor\", \"duracion_dias\", \"dias_firma_a_inicio\",\n",
    "    \"flag_rush\", \"flag_q4\", \"flag_short_contract\", \"flag_long_contract\",\n",
    "    \"is_direct\", \"is_modified\", \"flag_extreme_value\",\n",
    "    \"vendor_total_contracts\", \"vendor_direct_rate\", \"vendor_modified_rate\",\n",
    "    \"vendor_distinct_agencies\", \"vendor_agency_diversity\",\n",
    "    \"log_vendor_total_spend\", \"log_vendor_mean_value\",\n",
    "    \"agency_hhi\", \"agency_top_vendor_share\", \"flag_agency_concentrated\",\n",
    "    \"agency_direct_rate\", \"agency_modified_rate\", \"agency_distinct_vendors\",\n",
    "    \"vendor_tenure_days\", \"agency_median_value\"\n",
    "]\n",
    "\n",
    "available = [f for f in MODEL_FEATURES if f in fm.columns]\n",
    "X = fm[available].fillna(0)\n",
    "\n",
    "# Train a simple Random Forest as surrogate to get feature importances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sample 100K rows for speed\n",
    "sample = fm.sample(100000, random_state=42)\n",
    "X_sample = sample[available].fillna(0)\n",
    "y_sample = sample[\"proxy_strong\"]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_sample, y_sample)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=available).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 features by importance (Random Forest surrogate on proxy label):\")\n",
    "print(importances.head(15).round(4).to_string())\n",
    "\n",
    "print(f\"\\nTop 3 features account for: {importances.head(3).sum()*100:.1f}% of importance\")\n",
    "print(f\"Top 5 features account for: {importances.head(5).sum()*100:.1f}% of importance\")\n",
    "print(f\"\\nIf top 3 > 80% → possible overfitting on a few features\")\n",
    "print(f\"If top 3 < 60% → importance spread across features (healthy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1010180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CROSS-YEAR STABILITY ===\n",
      "Precision@K by year — should be consistent, not degrading\n",
      "\n",
      "  2019: Precision@5% = 19.8%  |  baseline = 22.1%  |  lift = 0.90x  |  n = 146,014\n",
      "  2020: Precision@5% = 29.8%  |  baseline = 16.4%  |  lift = 1.82x  |  n = 348,174\n",
      "  2021: Precision@5% = 27.5%  |  baseline = 14.4%  |  lift = 1.91x  |  n = 542,730\n",
      "  2022: Precision@5% = 37.2%  |  baseline = 15.5%  |  lift = 2.41x  |  n = 438,781\n",
      "\n",
      "If lift is consistent across years → model is not overfitting to a specific period\n",
      "If lift collapses in 2022 → potential overfitting\n"
     ]
    }
   ],
   "source": [
    "print(\"=== CROSS-YEAR STABILITY ===\")\n",
    "print(\"Precision@K by year — should be consistent, not degrading\\n\")\n",
    "\n",
    "for year in sorted(fm[\"year\"].unique()):\n",
    "    year_df = fm[fm[\"year\"] == year].assign(\n",
    "        tier_rank=lambda x: x[\"risk_tier\"].map({\"High\": 2, \"Medium\": 1, \"Low\": 0})\n",
    "    ).sort_values([\"tier_rank\", \"process_anomaly_norm\"], ascending=[False, False])\n",
    "    \n",
    "    k = int(len(year_df) * 0.05)\n",
    "    if k == 0:\n",
    "        continue\n",
    "    pak = year_df.head(k)[\"proxy_strong\"].mean()\n",
    "    baseline = year_df[\"proxy_strong\"].mean()\n",
    "    lift = pak / baseline if baseline > 0 else 0\n",
    "    print(f\"  {year}: Precision@5% = {pak*100:.1f}%  |  baseline = {baseline*100:.1f}%  |  lift = {lift:.2f}x  |  n = {len(year_df):,}\")\n",
    "\n",
    "print(\"\\nIf lift is consistent across years → model is not overfitting to a specific period\")\n",
    "print(\"If lift collapses in 2022 → potential overfitting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
