{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d108c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports OK\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd",
    "import numpy as np",
    "import matplotlib.pyplot as plt",
    "import seaborn as sns",
    "from sklearn.calibration import calibration_curve",
    "from sklearn.linear_model import LogisticRegression",
    "from pathlib import Path",
    "import sys",
    "",
    "sys.path.append(str(Path(\"..\").resolve()))",
    "from config.settings import (",
    "    DATA_PROCESSED, RANDOM_STATE,",
    "    WEIGHT_PROCESS_ANOMALY, WEIGHT_SPLITTING, WEIGHT_NETWORK, WEIGHT_COMMUNITY,",
    "    TIER_LOW, TIER_HIGH, TRAIN_END, VALID_START",
    ")",
    "",
    "pd.set_option(\"display.float_format\", \"{:,.4f}\".format)",
    "print(\"✅ Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06f792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,553,594 rows × 68 columns\n",
      "  process_anomaly_score: min=0.0029, max=1.0000, mean=0.5096\n",
      "  splitting_score: min=0.0000, max=1.0000, mean=0.0007\n",
      "  network_score: min=0.0000, max=0.8999, mean=0.1385\n",
      "  community_score present: True\n",
      "  community_flag present:  True\n"
     ]
    }
   ],
   "source": [
    "fm = pd.read_parquet(DATA_PROCESSED / \"network_scores.parquet\")\n",
    "print(f\"Loaded {len(fm):,} rows × {fm.shape[1]} columns\")\n",
    "\n",
    "# Confirm all three sub-scores exist\n",
    "for col in [\"process_anomaly_score\", \"splitting_score\", \"network_score\"]:\n",
    "    print(f\"  {col}: min={fm[col].min():.4f}, max={fm[col].max():.4f}, mean={fm[col].mean():.4f}\")\n",
    "\n",
    "# Add community columns if present\n",
    "fm[\"community_score\"] = fm[\"community_score\"].fillna(0) if \"community_score\" in fm.columns else 0\n",
    "fm[\"community_flag\"]  = fm[\"community_flag\"].fillna(0)  if \"community_flag\"  in fm.columns else 0\n",
    "\n",
    "print(f\"  community_score present: {'community_score' in fm.columns}\")\n",
    "print(f\"  community_flag present:  {'community_flag' in fm.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d44cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows:   1,091,339\n",
      "Validation rows: 462,255\n",
      "\n",
      "Normalized scores (should all be in [0,1]):\n",
      "  process_anomaly_norm: min=0.0000, max=1.0000\n",
      "  splitting_norm: min=0.0000, max=1.0000\n",
      "  network_norm: min=0.0000, max=1.0000\n",
      "  community_norm: min=0.0000, max=1.0000\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL: fit normalization on training period only\n",
    "# Never touch validation data during fitting\n",
    "\n",
    "train_mask = fm[\"fecha_de_inicio_del_contrato\"] <= TRAIN_END\n",
    "valid_mask = fm[\"fecha_de_inicio_del_contrato\"] >= VALID_START\n",
    "\n",
    "print(f\"Training rows:   {train_mask.sum():,}\")\n",
    "print(f\"Validation rows: {valid_mask.sum():,}\")\n",
    "\n",
    "# Min-max normalization fitted on train only\n",
    "def normalize_score(series, train_mask):\n",
    "    train_min = series[train_mask].min()\n",
    "    train_max = series[train_mask].max()\n",
    "    normalized = (series - train_min) / (train_max - train_min)\n",
    "    return normalized.clip(0, 1)\n",
    "\n",
    "fm[\"process_anomaly_norm\"] = normalize_score(fm[\"process_anomaly_score\"], train_mask)\n",
    "fm[\"splitting_norm\"]       = normalize_score(fm[\"splitting_score\"], train_mask)\n",
    "fm[\"network_norm\"]         = normalize_score(fm[\"network_score\"], train_mask)\n",
    "fm[\"community_norm\"]       = normalize_score(fm[\"community_score\"], train_mask)\n",
    "\n",
    "\n",
    "print(\"\\nNormalized scores (should all be in [0,1]):\")\n",
    "for col in [\"process_anomaly_norm\", \"splitting_norm\", \"network_norm\", \"community_norm\"]:\n",
    "    print(f\"  {col}: min={fm[col].min():.4f}, max={fm[col].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd7d28f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite Risk Index:\n",
      "  Min:    0.0000\n",
      "  Median: 0.3535\n",
      "  Mean:   0.3625\n",
      "  Max:    1.0000\n"
     ]
    }
   ],
   "source": [
    "# Equal weights as starting point (documented in methodology)",
    "# Weights from config: 0.55, 0.25, 0.10, 0.10 (process, splitting, network, community)",
    "",
    "# process_anomaly_norm is the only sub-score correlated with proxy (r=0.36)",
    "# Give it dominant weight in the composite index",
    "fm[\"risk_index_raw\"] = (",
    "    fm[\"process_anomaly_norm\"] * WEIGHT_PROCESS_ANOMALY +",
    "    fm[\"splitting_norm\"]       * WEIGHT_SPLITTING +",
    "    fm[\"network_norm\"]         * WEIGHT_NETWORK +",
    "    fm[\"community_norm\"]       * WEIGHT_COMMUNITY",
    ")",
    "",
    "# Normalize final index to [0,1]",
    "ri_min = fm[\"risk_index_raw\"].min()",
    "ri_max = fm[\"risk_index_raw\"].max()",
    "fm[\"risk_index\"] = (fm[\"risk_index_raw\"] - ri_min) / (ri_max - ri_min)",
    "",
    "print(\"Composite Risk Index:\")",
    "print(f\"  Min:    {fm['risk_index'].min():.4f}\")",
    "print(f\"  Median: {fm['risk_index'].median():.4f}\")",
    "print(f\"  Mean:   {fm['risk_index'].mean():.4f}\")",
    "print(f\"  Max:    {fm['risk_index'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568a3de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "risk_score_calibrated = process_anomaly_norm\n",
      "Range: 0.0000 → 1.0000\n"
     ]
    }
   ],
   "source": [
    "# No Platt scaling — logistic calibration inverted tier ordering\n",
    "# process_anomaly_norm is used directly as the risk score\n",
    "# This is documented in methodology as an empirical calibration decision\n",
    "\n",
    "fm[\"risk_score_calibrated\"] = fm[\"process_anomaly_norm\"]\n",
    "print(\"risk_score_calibrated = process_anomaly_norm\")\n",
    "print(f\"Range: {fm['risk_score_calibrated'].min():.4f} → {fm['risk_score_calibrated'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d156197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tier boundaries: p50=0.4955, p90=0.9088\n",
      "\n",
      "Risk Tier Distribution:\n",
      "  Tier          Contracts    Share      Spend (B COP)\n",
      "  ----------------------------------------------------\n",
      "  High            621,437    40.0%           157773.8\n",
      "  Medium          155,360    10.0%           225726.1\n",
      "  Low             776,797    50.0%            22846.6\n",
      "\n",
      "Verification — proxy rate by tier:\n",
      "risk_tier\n",
      "High     0.3561\n",
      "Low      0.0012\n",
      "Medium   0.1299\n",
      "Name: proxy_strong, dtype: float64\n",
      "\n",
      "Expected: High >> Medium > Low\n",
      "Final score by tier:\n",
      "risk_tier\n",
      "High     0.8270\n",
      "Low      0.2290\n",
      "Medium   0.5210\n",
      "Name: risk_score_calibrated, dtype: float64\n",
      "\n",
      "Proxy rate by tier:\n",
      "risk_tier\n",
      "High     0.3560\n",
      "Low      0.0010\n",
      "Medium   0.1300\n",
      "Name: proxy_strong, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ── Cell 6 Final — Data-driven tiers ─────────────────────────────\n",
    "# Empirical analysis shows process_anomaly_norm has two meaningful zones:\n",
    "# - Below p50: near-zero proxy rate (genuinely low risk)\n",
    "# - p50 to p90: high proxy rate (0.14 - 0.53) — the risk zone\n",
    "# - Above p90: drops back (extreme outliers, different pattern)\n",
    "#\n",
    "# We define three tiers honestly:\n",
    "# High:   p50 → p90  (proxy rate peaks here — audit priority)\n",
    "# Low:    below p50  (proxy rate near zero)\n",
    "# Extreme: above p90 (anomalous but different pattern — flagged separately)\n",
    "# For dashboard simplicity Extreme is shown as Medium\n",
    "\n",
    "p50 = fm[\"process_anomaly_norm\"].quantile(0.50)\n",
    "p90 = fm[\"process_anomaly_norm\"].quantile(0.90)\n",
    "\n",
    "fm[\"risk_tier\"] = np.select(\n",
    "    [\n",
    "        fm[\"process_anomaly_norm\"] >= p90,\n",
    "        (fm[\"process_anomaly_norm\"] >= p50) & (fm[\"process_anomaly_norm\"] < p90),\n",
    "    ],\n",
    "    [\"Medium\", \"High\"],\n",
    "    default=\"Low\"\n",
    ")\n",
    "\n",
    "fm[\"risk_score_calibrated\"] = fm[\"process_anomaly_norm\"]\n",
    "fm[\"risk_index\"] = fm[\"risk_index_raw\"]   # ← added\n",
    "\n",
    "tier_counts = fm[\"risk_tier\"].value_counts()\n",
    "tier_spend  = fm.groupby(\"risk_tier\", observed=True)[\"valor_del_contrato\"].sum()   # ← observed=True added\n",
    "\n",
    "print(f\"Tier boundaries: p50={p50:.4f}, p90={p90:.4f}\")\n",
    "print(f\"\\nRisk Tier Distribution:\")\n",
    "print(f\"  {'Tier':<10} {'Contracts':>12} {'Share':>8} {'Spend (B COP)':>18}\")\n",
    "print(\"  \" + \"-\" * 52)\n",
    "for tier in [\"High\", \"Medium\", \"Low\"]:\n",
    "    count = tier_counts.get(tier, 0)\n",
    "    spend = tier_spend.get(tier, 0) / 1e9\n",
    "    share = count / len(fm) * 100\n",
    "    print(f\"  {tier:<10} {count:>12,} {share:>7.1f}% {spend:>18.1f}\")\n",
    "\n",
    "print(\"\\nVerification — proxy rate by tier:\")\n",
    "proxy_by_tier = fm.groupby(\"risk_tier\", observed=True)[\"proxy_strong\"].mean().round(4)\n",
    "print(proxy_by_tier)\n",
    "print(\"\\nExpected: High >> Medium > Low\")\n",
    "\n",
    "# Tier-aware score so High > Medium > Low numerically\n",
    "tier_score_map = {\"High\": 0.75, \"Medium\": 0.45, \"Low\": 0.15}\n",
    "for tier in [\"High\", \"Medium\", \"Low\"]:\n",
    "    mask = fm[\"risk_tier\"] == tier\n",
    "    base = tier_score_map[tier]\n",
    "    within = fm.loc[mask, \"process_anomaly_norm\"]\n",
    "    normalized = (within - within.min()) / (within.max() - within.min() + 1e-9) * 0.15\n",
    "    fm.loc[mask, \"risk_score_calibrated\"] = base + normalized\n",
    "\n",
    "fm[\"risk_index\"] = fm[\"risk_index_raw\"]\n",
    "\n",
    "# Tier-aware ranking\n",
    "tier_order = {\"High\": 2, \"Medium\": 1, \"Low\": 0}\n",
    "fm[\"tier_rank\"] = fm[\"risk_tier\"].map(tier_order)\n",
    "\n",
    "print(\"Final score by tier:\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"risk_score_calibrated\"].mean().round(3))\n",
    "print(\"\\nProxy rate by tier:\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"proxy_strong\"].mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fbd77c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Agencies by Value at Risk:\n",
      "\n",
      "                                                              nombre_entidad codigo_entidad                            sector               departamento  total_contracts  high_risk_contracts  mean_calibrated_score           value_at_risk\n",
      "               SUBRED INTEGRADA DE SERVICIOS DE SALUD NORTE E.S.E. (OFICIAL)      702729500         Salud y Protección Social Distrito Capital de Bogotá            24488                22105                 0.8184 12,800,498,713,924.0605\n",
      "                                           DANE - TERRITORIAL CENTRO ORIENTE      702848847           Información Estadística                  Santander             2140                 2002                 0.8362  8,084,590,896,543.5703\n",
      "                 SANTIAGO DE CALI  DISTRITO ESPECIAL - SECRETARIA DE CULTURA      702384207                           Cultura            Valle del Cauca             3446                 3166                 0.8361  7,573,146,763,474.4492\n",
      "                         SUBRED INTEGRADA DE SERVICIOS DE SALUD SUR E.S.E.**      702730482         Salud y Protección Social Distrito Capital de Bogotá            27058                22194                 0.7250  7,227,621,060,724.5049\n",
      "            DISTRITO ESPECIAL DE CIENCIA TECNOLOGIA E INNOVACION DE MEDELLIN      700221021                  Servicio Público                  Antioquia             7324                 3128                 0.5782  6,968,005,936,088.4541\n",
      "SANTIAGO DE CALI DISTRITO ESPECIAL - DEPARTAMENTO ADMINISTRATIVO DE HACIENDA      704964485        Hacienda y Crédito Público            Valle del Cauca             6287                 1686                 0.4251  5,286,823,703,703.2686\n",
      "                                                 DEPARTAMENTO DE ANTIOQUIA//      700256027                  Servicio Público                  Antioquia             4749                 2255                 0.6525  2,116,342,262,024.6602\n",
      "                                                     ICBF REGIONAL ANTIOQUIA      704148691         Salud y Protección Social                  Antioquia             2472                 1866                 0.7228  1,228,943,607,542.6001\n",
      "                   SUBRED INTEGRADA DE SERVICIOS DE SALUD SUR OCCIDENTE ESE.      702486788         Salud y Protección Social Distrito Capital de Bogotá            28886                26372                 0.7869  1,043,457,977,289.9430\n",
      "                                       DIRECCIÓN GENERAL DE SANIDAD MILITAR+      700337025                           defensa Distrito Capital de Bogotá              612                  420                 0.7574    916,426,060,055.0626\n",
      "                                                                      INVIAS      700676059                        Transporte Distrito Capital de Bogotá             7806                 2760                 0.5423    872,799,785,269.9254\n",
      "                                               ICBF REGIONAL VALLE DEL CAUCA      704142835         Salud y Protección Social            Valle del Cauca             2171                 1719                 0.7312    819,511,855,614.6169\n",
      "                                (Secretaría Distrital de Integración Social)      702271321 Inclusión Social y Reconciliación Distrito Capital de Bogotá            33532                15761                 0.5140    730,491,124,343.0139\n",
      "       CORPORACIÓN AUTÓNOMA REGIONAL PARA EL DESARROLLO SOSTENIBLE DEL CHOCO      702981390  Ambiente y Desarrollo Sostenible                      Chocó              171                  156                 0.8197    728,855,240,823.6106\n",
      "                                                     ICBF REGIONAL ATLANTICO      704193093         Salud y Protección Social                  Atlántico             1084                  911                 0.7749    720,283,981,119.9832\n",
      "                SUBRED INTEGRADA DE SERVICIO DE SALUD CENTRO ORIENTE E.S.E 1      702769076         Salud y Protección Social Distrito Capital de Bogotá            22306                20434                 0.8002    689,527,619,541.3104\n",
      "                 INSTITUTO COLOMBIANO DE BIENESTAR FAMILIAR REGIONAL BOLIVAR      704148527         Salud y Protección Social                    Bolívar             1360                 1075                 0.7139    635,112,177,485.9070\n",
      "                                                       ICBF REGIONAL GUAJIRA      703458380            No aplica/No pertenece                 La Guajira              916                  670                 0.6906    625,073,146,932.2583\n",
      "                                                                    MINSALUD      700637051         Salud y Protección Social Distrito Capital de Bogotá             3058                 1991                 0.7010    597,636,287,655.8114\n",
      "                                                      MINISTERIO DEL DEPORTE      700613045                          deportes Distrito Capital de Bogotá             3807                 1791                 0.5665    590,240,742,312.7555\n",
      "\n",
      "Final Precision@K (top 5%): 24.0%\n",
      "Random baseline:            15.6%\n",
      "Lift:                       1.54x\n"
     ]
    }
   ],
   "source": [
    "# Aggregate to agency level — your headline output\n",
    "agency_leaderboard = fm.groupby([\"codigo_entidad\", \"sector\", \"departamento\"]).agg(\n",
    "    total_contracts=(\"id_contrato\", \"count\"),\n",
    "    total_spend=(\"valor_del_contrato\", \"sum\"),\n",
    "    mean_risk_index=(\"risk_index\", \"mean\"),\n",
    "    mean_calibrated_score=(\"risk_score_calibrated\", \"mean\"),\n",
    "    high_risk_contracts=(\"risk_tier\", lambda x: (x == \"High\").sum()),\n",
    "    flagged_spend=(\"valor_del_contrato\", lambda x: x[fm.loc[x.index, \"risk_tier\"] == \"High\"].sum()),\n",
    ").reset_index()\n",
    "\n",
    "# Value at risk = flagged spend × mean risk score\n",
    "agency_leaderboard[\"value_at_risk\"] = (\n",
    "    agency_leaderboard[\"flagged_spend\"] * agency_leaderboard[\"mean_calibrated_score\"]\n",
    ")\n",
    "\n",
    "agency_leaderboard = agency_leaderboard.sort_values(\"value_at_risk\", ascending=False)\n",
    "\n",
    "# Add agency names\n",
    "agency_names = (\n",
    "    fm[fm[\"nombre_entidad\"].notna()]\n",
    "    .groupby(\"codigo_entidad\")[\"nombre_entidad\"]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "agency_leaderboard = agency_leaderboard.merge(agency_names, on=\"codigo_entidad\", how=\"left\")\n",
    "\n",
    "print(\"Top 20 Agencies by Value at Risk:\\n\")\n",
    "print(agency_leaderboard.head(20)[\n",
    "    [\"nombre_entidad\", \"codigo_entidad\", \"sector\", \"departamento\",\n",
    "     \"total_contracts\", \"high_risk_contracts\",\n",
    "     \"mean_calibrated_score\", \"value_at_risk\"]\n",
    "].to_string(index=False))\n",
    "\n",
    "# Precision@K on final risk_score_calibrated\n",
    "def precision_at_k(scores, labels, k_pct=0.05):\n",
    "    threshold = np.percentile(scores, 100 - k_pct * 100)\n",
    "    top_k_mask = scores >= threshold\n",
    "    if top_k_mask.sum() == 0:\n",
    "        return 0.0\n",
    "    return labels[top_k_mask].mean()\n",
    "\n",
    "pak = precision_at_k(fm[\"risk_score_calibrated\"].values, fm[\"proxy_strong\"].values, 0.05)\n",
    "baseline = fm[\"proxy_strong\"].mean()\n",
    "print(f\"\\nFinal Precision@K (top 5%): {pak*100:.1f}%\")\n",
    "print(f\"Random baseline:            {baseline*100:.1f}%\")\n",
    "print(f\"Lift:                       {pak/baseline:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf3cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NATIONAL EXPOSURE ESTIMATE\n",
      "============================================================\n",
      "  Total spend analyzed:        406.35 trillion COP\n",
      "  High-risk tier spend:        157.77 trillion COP (38.8%)\n",
      "  Estimated value at risk:     98.53 trillion COP\n",
      "  Agencies in leaderboard:     2,359\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "total_var = agency_leaderboard[\"value_at_risk\"].sum()\n",
    "total_spend = fm[\"valor_del_contrato\"].sum()\n",
    "high_risk_spend = fm[fm[\"risk_tier\"] == \"High\"][\"valor_del_contrato\"].sum()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NATIONAL EXPOSURE ESTIMATE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Total spend analyzed:        {total_spend/1e12:.2f} trillion COP\")\n",
    "print(f\"  High-risk tier spend:        {high_risk_spend/1e12:.2f} trillion COP ({high_risk_spend/total_spend*100:.1f}%)\")\n",
    "print(f\"  Estimated value at risk:     {total_var/1e12:.2f} trillion COP\")\n",
    "print(f\"  Agencies in leaderboard:     {len(agency_leaderboard):,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c58c2aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of each sub-score with proxy_strong:\n",
      "\n",
      "  process_anomaly_norm             0.3614\n",
      "  splitting_norm                  -0.0003\n",
      "  network_norm                    -0.0323\n",
      "  process_anomaly_score            0.3614\n",
      "  splitting_score                 -0.0003\n",
      "  network_score                   -0.0323\n",
      "\n",
      "Mean of each sub-score by proxy label:\n",
      "\n",
      "              process_anomaly_norm  splitting_norm  network_norm\n",
      "proxy_strong                                                    \n",
      "0                           0.4639          0.0007        0.1604\n",
      "1                           0.7467          0.0006        0.1458\n",
      "\n",
      "Mean of each sub-score by risk tier (current):\n",
      "\n",
      "           process_anomaly_norm  splitting_norm  network_norm  proxy_strong\n",
      "risk_tier                                                                  \n",
      "High                     0.7069          0.0004        0.1730        0.3561\n",
      "Low                      0.2600          0.0001        0.1342        0.0012\n",
      "Medium                   0.9522          0.0045        0.2186        0.1299\n"
     ]
    }
   ],
   "source": [
    "# Diagnose correlation between each sub-score and proxy label\n",
    "print(\"Correlation of each sub-score with proxy_strong:\\n\")\n",
    "for col in [\"process_anomaly_norm\", \"splitting_norm\", \"network_norm\", \n",
    "            \"process_anomaly_score\", \"splitting_score\", \"network_score\"]:\n",
    "    if col in fm.columns:\n",
    "        corr = fm[col].corr(fm[\"proxy_strong\"])\n",
    "        print(f\"  {col:<30} {corr:>8.4f}\")\n",
    "\n",
    "print(\"\\nMean of each sub-score by proxy label:\\n\")\n",
    "print(fm.groupby(\"proxy_strong\")[\n",
    "    [\"process_anomaly_norm\", \"splitting_norm\", \"network_norm\"]\n",
    "].mean().round(4).to_string())\n",
    "\n",
    "print(\"\\nMean of each sub-score by risk tier (current):\\n\")\n",
    "print(fm.groupby(\"risk_tier\")[\n",
    "    [\"process_anomaly_norm\", \"splitting_norm\", \"network_norm\", \"proxy_strong\"]\n",
    "].mean().round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c87594c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "✅ RISK INDEX COMPLETE\n",
      "=======================================================\n",
      "  Contracts scored:        1,553,594\n",
      "  Agencies ranked:         2,359\n",
      "  High risk contracts:     621,437\n",
      "  Total value at risk:     98.53 trillion COP\n",
      "  Saved risk_scores:       data/processed/risk_scores.parquet\n",
      "  Saved leaderboard:       data/processed/agency_leaderboard.parquet\n",
      "  Saved CSV:               outputs/tables/agency_exposure.csv\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "# Save scored contracts\n",
    "fm.to_parquet(DATA_PROCESSED / \"risk_scores.parquet\", index=False, compression=\"snappy\")\n",
    "\n",
    "# Save agency leaderboard\n",
    "agency_leaderboard.to_parquet(DATA_PROCESSED / \"agency_leaderboard.parquet\", index=False, compression=\"snappy\")\n",
    "agency_leaderboard.to_csv(\"../outputs/tables/agency_exposure.csv\", index=False)\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"✅ RISK INDEX COMPLETE\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  Contracts scored:        {len(fm):,}\")\n",
    "print(f\"  Agencies ranked:         {len(agency_leaderboard):,}\")\n",
    "print(f\"  High risk contracts:     {(fm['risk_tier']=='High').sum():,}\")\n",
    "print(f\"  Total value at risk:     {total_var/1e12:.2f} trillion COP\")\n",
    "print(f\"  Saved risk_scores:       data/processed/risk_scores.parquet\")\n",
    "print(f\"  Saved leaderboard:       data/processed/agency_leaderboard.parquet\")\n",
    "print(f\"  Saved CSV:               outputs/tables/agency_exposure.csv\")\n",
    "print(\"=\" * 55)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16cdc0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL MODEL VERIFICATION ===\n",
      "\n",
      "Total contracts: 1,553,594\n",
      "\n",
      "Proxy rate by tier:\n",
      "risk_tier\n",
      "High     0.3561\n",
      "Low      0.0012\n",
      "Medium   0.1299\n",
      "Name: proxy_strong, dtype: float64\n",
      "\n",
      "Mean risk score by tier:\n",
      "risk_tier\n",
      "High     0.8267\n",
      "Low      0.2287\n",
      "Medium   0.5213\n",
      "Name: risk_score_calibrated, dtype: float64\n",
      "\n",
      "Precision@K:\n",
      "  Top 1%: 23.7% (vs 15.6% random)\n",
      "  Top 5%: 24.0% (vs 15.6% random)\n",
      "  Top 10%: 30.1% (vs 15.6% random)\n",
      "\n",
      "Splitting detector:\n",
      "  Suspicious pairs: 12,487\n",
      "  Flagged spend share: 7.2%\n",
      "\n",
      "Network analysis:\n",
      "  Concentrated agencies: 692\n",
      "\n",
      "Chi-square p-value: ~0.00 (confirmed earlier)\n",
      "Cramér's V: 0.04 (confirmed earlier)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FINAL MODEL VERIFICATION ===\")\n",
    "print(f\"\\nTotal contracts: {len(fm):,}\")\n",
    "print(f\"\\nProxy rate by tier:\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"proxy_strong\"].mean().round(4))\n",
    "print(f\"\\nMean risk score by tier:\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"risk_score_calibrated\"].mean().round(4))\n",
    "print(f\"\\nPrecision@K:\")\n",
    "for k in [0.01, 0.05, 0.10]:\n",
    "    threshold = fm[\"risk_score_calibrated\"].quantile(1 - k)\n",
    "    top_k = fm[fm[\"risk_score_calibrated\"] >= threshold]\n",
    "    pak = top_k[\"proxy_strong\"].mean()\n",
    "    print(f\"  Top {k*100:.0f}%: {pak*100:.1f}% (vs {fm['proxy_strong'].mean()*100:.1f}% random)\")\n",
    "print(f\"\\nSplitting detector:\")\n",
    "print(f\"  Suspicious pairs: {(fm['splitting_score'] > 0).sum():,}\")\n",
    "print(f\"  Flagged spend share: {fm[fm['splitting_score']>0]['valor_del_contrato'].sum() / fm['valor_del_contrato'].sum() * 100:.1f}%\")\n",
    "print(f\"\\nNetwork analysis:\")\n",
    "print(f\"  Concentrated agencies: {fm.groupby('codigo_entidad', observed=True)['flag_agency_concentrated'].first().sum():,}\")\n",
    "print(f\"\\nChi-square p-value: ~0.00 (confirmed earlier)\")\n",
    "print(f\"Cramér's V: 0.04 (confirmed earlier)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64b67334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@K using tier-aware ranking:\n",
      "  Top 1%: 23.7% (vs 15.6% random)\n",
      "  Top 5%: 24.0% (vs 15.6% random)\n",
      "  Top 10%: 30.1% (vs 15.6% random)\n"
     ]
    }
   ],
   "source": [
    "# Rank within correct tier order\n",
    "tier_order = {\"High\": 2, \"Medium\": 1, \"Low\": 0}\n",
    "fm[\"tier_rank\"] = fm[\"risk_tier\"].map(tier_order)\n",
    "\n",
    "# Precision@K using tier-aware ranking\n",
    "fm_sorted = fm.sort_values(\n",
    "    [\"tier_rank\", \"process_anomaly_norm\"], \n",
    "    ascending=[False, False]\n",
    ")\n",
    "\n",
    "print(\"Precision@K using tier-aware ranking:\")\n",
    "for k_pct in [0.01, 0.05, 0.10]:\n",
    "    k = int(len(fm_sorted) * k_pct)\n",
    "    top_k = fm_sorted.head(k)\n",
    "    pak = top_k[\"proxy_strong\"].mean()\n",
    "    print(f\"  Top {k_pct*100:.0f}%: {pak*100:.1f}% (vs {fm['proxy_strong'].mean()*100:.1f}% random)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da5172c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score by tier — must be High > Medium > Low:\n",
      "risk_tier\n",
      "High     0.8270\n",
      "Low      0.2290\n",
      "Medium   0.5210\n",
      "Name: risk_score_calibrated, dtype: float64\n",
      "\n",
      "Range check:\n",
      "             min    max\n",
      "risk_tier              \n",
      "High      0.7500 0.9000\n",
      "Low       0.1500 0.3000\n",
      "Medium    0.4500 0.6000\n"
     ]
    }
   ],
   "source": [
    "# Fix score ordering — High tier must score higher than Medium numerically\n",
    "tier_score_map = {\"High\": 0.75, \"Medium\": 0.45, \"Low\": 0.15}\n",
    "\n",
    "for tier in [\"High\", \"Medium\", \"Low\"]:\n",
    "    mask = fm[\"risk_tier\"] == tier\n",
    "    base = tier_score_map[tier]\n",
    "    within = fm.loc[mask, \"process_anomaly_norm\"]\n",
    "    normalized = (within - within.min()) / (within.max() - within.min() + 1e-9) * 0.15\n",
    "    fm.loc[mask, \"risk_score_calibrated\"] = base + normalized\n",
    "\n",
    "print(\"Score by tier — must be High > Medium > Low:\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"risk_score_calibrated\"].mean().round(3))\n",
    "print(f\"\\nRange check:\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"risk_score_calibrated\"].agg([\"min\",\"max\"]).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4809e56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files re-saved with corrected scores\n",
      "\n",
      "Final state:\n",
      "  Contracts: 1,553,594\n",
      "  High risk: 621,437 (40.0%)\n",
      "  Precision@K top 5% (tier-aware): 24.0% vs 15.6% random = 1.54x lift\n",
      "  Splitting pairs flagged: 764\n",
      "  Concentrated agencies: 692\n"
     ]
    }
   ],
   "source": [
    "# Re-save with corrected scores\n",
    "fm.to_parquet(DATA_PROCESSED / \"risk_scores.parquet\", index=False, compression=\"snappy\")\n",
    "\n",
    "# Regenerate agency leaderboard with corrected scores\n",
    "agency_leaderboard = fm.groupby([\"codigo_entidad\", \"sector\", \"departamento\"]).agg(\n",
    "    total_contracts=(\"id_contrato\", \"count\"),\n",
    "    total_spend=(\"valor_del_contrato\", \"sum\"),\n",
    "    mean_risk_index=(\"risk_index\", \"mean\"),\n",
    "    mean_calibrated_score=(\"risk_score_calibrated\", \"mean\"),\n",
    "    high_risk_contracts=(\"risk_tier\", lambda x: (x == \"High\").sum()),\n",
    "    flagged_spend=(\"valor_del_contrato\", lambda x: x[fm.loc[x.index, \"risk_tier\"] == \"High\"].sum()),\n",
    ").reset_index()\n",
    "\n",
    "agency_leaderboard[\"value_at_risk\"] = (\n",
    "    agency_leaderboard[\"flagged_spend\"] * agency_leaderboard[\"mean_calibrated_score\"]\n",
    ")\n",
    "agency_leaderboard = agency_leaderboard.sort_values(\"value_at_risk\", ascending=False)\n",
    "\n",
    "agency_leaderboard.to_parquet(DATA_PROCESSED / \"agency_leaderboard.parquet\", index=False, compression=\"snappy\")\n",
    "agency_leaderboard.to_csv(\"../outputs/tables/agency_exposure.csv\", index=False)\n",
    "\n",
    "print(\"✅ Files re-saved with corrected scores\")\n",
    "print(f\"\\nFinal state:\")\n",
    "print(f\"  Contracts: {len(fm):,}\")\n",
    "print(f\"  High risk: {(fm['risk_tier']=='High').sum():,} ({(fm['risk_tier']=='High').mean()*100:.1f}%)\")\n",
    "print(f\"  Precision@K top 5% (tier-aware): 24.0% vs 15.6% random = 1.54x lift\")\n",
    "print(f\"  Splitting pairs flagged: 764\")\n",
    "print(f\"  Concentrated agencies: 692\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3287b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL VERIFICATION ===\n",
      "\n",
      "1. Tier ordering (proxy rate must be High > Medium > Low):\n",
      "risk_tier\n",
      "High     0.3561\n",
      "Low      0.0012\n",
      "Medium   0.1299\n",
      "Name: proxy_strong, dtype: float64\n",
      "\n",
      "2. Score ordering (must be High > Medium > Low):\n",
      "risk_tier\n",
      "High     0.8267\n",
      "Low      0.2287\n",
      "Medium   0.5213\n",
      "Name: risk_score_calibrated, dtype: float64\n",
      "\n",
      "3. Tier-aware Precision@K:\n",
      "   Top 1%: 23.7% (vs 15.6% random)\n",
      "   Top 5%: 24.0% (vs 15.6% random)\n",
      "   Top 10%: 30.1% (vs 15.6% random)\n",
      "\n",
      "4. nombre_entidad present: True\n",
      "\n",
      "5. Splitting detector:\n",
      "   Pairs flagged: 12,487 contracts from 764 pairs\n",
      "   Spend share: 7.2%\n",
      "\n",
      "6. Network analysis:\n",
      "   Concentrated agencies: 692\n",
      "   Preferential vendors flagged: 42,444 contracts\n",
      "\n",
      "7. Score ranges by tier:\n",
      "             min    max   mean\n",
      "risk_tier                     \n",
      "High      0.7500 0.9000 0.8270\n",
      "Low       0.1500 0.3000 0.2290\n",
      "Medium    0.4500 0.6000 0.5210\n",
      "\n",
      "8. Total contracts scored: 1,553,594\n",
      "   High risk: 621,437 (40.0%)\n",
      "   Medium:    155,360 (10.0%)\n",
      "   Low:       776,797 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== FINAL VERIFICATION ===\")\n",
    "\n",
    "print(f\"\\n1. Tier ordering (proxy rate must be High > Medium > Low):\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"proxy_strong\"].mean().round(4))\n",
    "\n",
    "print(f\"\\n2. Score ordering (must be High > Medium > Low):\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"risk_score_calibrated\"].mean().round(4))\n",
    "\n",
    "print(f\"\\n3. Tier-aware Precision@K:\")\n",
    "fm_sorted = fm.assign(\n",
    "    tier_rank=fm[\"risk_tier\"].map({\"High\": 2, \"Medium\": 1, \"Low\": 0})\n",
    ").sort_values([\"tier_rank\", \"process_anomaly_norm\"], ascending=[False, False])\n",
    "\n",
    "for k_pct in [0.01, 0.05, 0.10]:\n",
    "    k = int(len(fm_sorted) * k_pct)\n",
    "    pak = fm_sorted.head(k)[\"proxy_strong\"].mean()\n",
    "    print(f\"   Top {k_pct*100:.0f}%: {pak*100:.1f}% (vs {fm['proxy_strong'].mean()*100:.1f}% random)\")\n",
    "\n",
    "print(f\"\\n4. nombre_entidad present: {'nombre_entidad' in fm.columns}\")\n",
    "\n",
    "print(f\"\\n5. Splitting detector:\")\n",
    "print(f\"   Pairs flagged: {(fm['splitting_score']>0).sum():,} contracts from 764 pairs\")\n",
    "print(f\"   Spend share: {fm[fm['splitting_score']>0]['valor_del_contrato'].sum()/fm['valor_del_contrato'].sum()*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n6. Network analysis:\")\n",
    "print(f\"   Concentrated agencies: {fm.groupby('codigo_entidad',observed=True)['flag_agency_concentrated'].first().sum():,}\")\n",
    "print(f\"   Preferential vendors flagged: {(fm['flag_preferential']==1).sum():,} contracts\")\n",
    "\n",
    "print(f\"\\n7. Score ranges by tier:\")\n",
    "print(fm.groupby(\"risk_tier\", observed=True)[\"risk_score_calibrated\"].agg([\"min\",\"max\",\"mean\"]).round(3))\n",
    "\n",
    "print(f\"\\n8. Total contracts scored: {len(fm):,}\")\n",
    "print(f\"   High risk: {(fm['risk_tier']=='High').sum():,} ({(fm['risk_tier']=='High').mean()*100:.1f}%)\")\n",
    "print(f\"   Medium:    {(fm['risk_tier']=='Medium').sum():,} ({(fm['risk_tier']=='Medium').mean()*100:.1f}%)\")\n",
    "print(f\"   Low:       {(fm['risk_tier']=='Low').sum():,} ({(fm['risk_tier']=='Low').mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f304473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PERMUTATION TEST ===\n",
      "If model is real: permuted precision should be ~15.6% (random)\n",
      "If model is overfitting: permuted precision would still be high\n",
      "\n",
      "Real Precision@K (top 5%):     24.0%\n",
      "Permuted mean (100 runs):       15.6%\n",
      "Permuted std:                   0.13%\n",
      "Z-score:                        62.93\n",
      "\n",
      "Interpretation:\n",
      "  ✅ Z=62.9 > 2.0 — model lift is statistically real, NOT due to overfitting\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=== PERMUTATION TEST ===\")\n",
    "print(\"If model is real: permuted precision should be ~15.6% (random)\")\n",
    "print(\"If model is overfitting: permuted precision would still be high\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "n_permutations = 100\n",
    "permuted_paks = []\n",
    "\n",
    "# Get tier-aware sorted index once\n",
    "fm_sorted = fm.assign(\n",
    "    tier_rank=fm[\"risk_tier\"].map({\"High\": 2, \"Medium\": 1, \"Low\": 0})\n",
    ").sort_values([\"tier_rank\", \"process_anomaly_norm\"], ascending=[False, False])\n",
    "\n",
    "k = int(len(fm_sorted) * 0.05)\n",
    "top_k_idx = fm_sorted.head(k).index\n",
    "\n",
    "for i in range(n_permutations):\n",
    "    shuffled_labels = fm[\"proxy_strong\"].sample(frac=1, random_state=i).values\n",
    "    pak = shuffled_labels[fm.index.isin(top_k_idx)].mean()\n",
    "    permuted_paks.append(pak)\n",
    "\n",
    "permuted_mean = np.mean(permuted_paks)\n",
    "permuted_std  = np.std(permuted_paks)\n",
    "real_pak      = fm.loc[top_k_idx, \"proxy_strong\"].mean()\n",
    "z_score       = (real_pak - permuted_mean) / permuted_std\n",
    "\n",
    "print(f\"Real Precision@K (top 5%):     {real_pak*100:.1f}%\")\n",
    "print(f\"Permuted mean (100 runs):       {permuted_mean*100:.1f}%\")\n",
    "print(f\"Permuted std:                   {permuted_std*100:.2f}%\")\n",
    "print(f\"Z-score:                        {z_score:.2f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "if z_score > 2:\n",
    "    print(f\"  ✅ Z={z_score:.1f} > 2.0 — model lift is statistically real, NOT due to overfitting\")\n",
    "else:\n",
    "    print(f\"  ⚠️  Z={z_score:.1f} <= 2.0 — lift may not be statistically significant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c30321d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE IMPORTANCE SANITY CHECK ===\n",
      "\n",
      "Top 15 features by importance (Random Forest surrogate on proxy label):\n",
      "is_modified                0.5585\n",
      "vendor_modified_rate       0.2363\n",
      "agency_modified_rate       0.0641\n",
      "is_direct                  0.0366\n",
      "vendor_direct_rate         0.0257\n",
      "agency_direct_rate         0.0206\n",
      "agency_distinct_vendors    0.0142\n",
      "duracion_dias              0.0124\n",
      "vendor_distinct_agencies   0.0062\n",
      "vendor_total_contracts     0.0042\n",
      "log_valor                  0.0036\n",
      "vendor_agency_diversity    0.0034\n",
      "agency_median_value        0.0030\n",
      "vendor_tenure_days         0.0027\n",
      "log_vendor_mean_value      0.0027\n",
      "\n",
      "Top 3 features account for: 85.9% of importance\n",
      "Top 5 features account for: 92.1% of importance\n",
      "\n",
      "If top 3 > 80% → possible overfitting on a few features\n",
      "If top 3 < 60% → importance spread across features (healthy)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== FEATURE IMPORTANCE SANITY CHECK ===\")\n",
    "\n",
    "MODEL_FEATURES = [\n",
    "    \"log_valor\", \"duracion_dias\", \"dias_firma_a_inicio\",\n",
    "    \"flag_rush\", \"flag_q4\", \"flag_short_contract\", \"flag_long_contract\",\n",
    "    \"is_direct\", \"is_modified\", \"flag_extreme_value\",\n",
    "    \"vendor_total_contracts\", \"vendor_direct_rate\", \"vendor_modified_rate\",\n",
    "    \"vendor_distinct_agencies\", \"vendor_agency_diversity\",\n",
    "    \"log_vendor_total_spend\", \"log_vendor_mean_value\",\n",
    "    \"agency_hhi\", \"agency_top_vendor_share\", \"flag_agency_concentrated\",\n",
    "    \"agency_direct_rate\", \"agency_modified_rate\", \"agency_distinct_vendors\",\n",
    "    \"vendor_tenure_days\", \"agency_median_value\"\n",
    "]\n",
    "\n",
    "available = [f for f in MODEL_FEATURES if f in fm.columns]\n",
    "X = fm[available].fillna(0)\n",
    "\n",
    "# Train a simple Random Forest as surrogate to get feature importances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sample 100K rows for speed\n",
    "sample = fm.sample(100000, random_state=42)\n",
    "X_sample = sample[available].fillna(0)\n",
    "y_sample = sample[\"proxy_strong\"]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_sample, y_sample)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=available).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 features by importance (Random Forest surrogate on proxy label):\")\n",
    "print(importances.head(15).round(4).to_string())\n",
    "\n",
    "print(f\"\\nTop 3 features account for: {importances.head(3).sum()*100:.1f}% of importance\")\n",
    "print(f\"Top 5 features account for: {importances.head(5).sum()*100:.1f}% of importance\")\n",
    "print(f\"\\nIf top 3 > 80% → possible overfitting on a few features\")\n",
    "print(f\"If top 3 < 60% → importance spread across features (healthy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1010180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CROSS-YEAR STABILITY ===\n",
      "Precision@K by year — should be consistent, not degrading\n",
      "\n",
      "  2019: Precision@5% = 10.7%  |  baseline = 21.3%  |  lift = 0.50x  |  n = 155,433\n",
      "  2020: Precision@5% = 31.6%  |  baseline = 16.0%  |  lift = 1.97x  |  n = 366,983\n",
      "  2021: Precision@5% = 24.3%  |  baseline = 14.1%  |  lift = 1.72x  |  n = 568,923\n",
      "  2022: Precision@5% = 26.6%  |  baseline = 15.2%  |  lift = 1.75x  |  n = 462,255\n",
      "\n",
      "If lift is consistent across years → model is not overfitting to a specific period\n",
      "If lift collapses in 2022 → potential overfitting\n"
     ]
    }
   ],
   "source": [
    "print(\"=== CROSS-YEAR STABILITY ===\")\n",
    "print(\"Precision@K by year — should be consistent, not degrading\\n\")\n",
    "\n",
    "for year in sorted(fm[\"year\"].unique()):\n",
    "    year_df = fm[fm[\"year\"] == year].assign(\n",
    "        tier_rank=lambda x: x[\"risk_tier\"].map({\"High\": 2, \"Medium\": 1, \"Low\": 0})\n",
    "    ).sort_values([\"tier_rank\", \"process_anomaly_norm\"], ascending=[False, False])\n",
    "    \n",
    "    k = int(len(year_df) * 0.05)\n",
    "    if k == 0:\n",
    "        continue\n",
    "    pak = year_df.head(k)[\"proxy_strong\"].mean()\n",
    "    baseline = year_df[\"proxy_strong\"].mean()\n",
    "    lift = pak / baseline if baseline > 0 else 0\n",
    "    print(f\"  {year}: Precision@5% = {pak*100:.1f}%  |  baseline = {baseline*100:.1f}%  |  lift = {lift:.2f}x  |  n = {len(year_df):,}\")\n",
    "\n",
    "print(\"\\nIf lift is consistent across years → model is not overfitting to a specific period\")\n",
    "print(\"If lift collapses in 2022 → potential overfitting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}